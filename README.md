# ğŸ›¡ï¸ Vigilix â€“ Intrusion Detection & Monitoring System

Welcome to **Vigilix**, an intelligent anomaly detection pipeline designed to identify and monitor network intrusions in real-time. This project leverages machine learning models, real-time data streaming, and monitoring tools to provide a robust and scalable solution for anomaly detection.

---

## ğŸ“š Overview

**Vigilix** is a modular and production-ready system that combines classical machine learning models with real-time monitoring capabilities. It is designed to process network data, detect anomalies, and visualize system performance metrics in real-time.

### Key Features:
- **Anomaly Detection Models:** Implements Isolation Forest, Random Forest, and XGBoost for anomaly detection.
- **Real-Time Data Streaming:** Uses Kafka for real-time data ingestion and processing.
- **Monitoring Stack:** Integrates Prometheus and Grafana for real-time metrics visualization.
- **Exploratory Data Analysis (EDA):** Provides insights into the dataset with statistical summaries and visualizations.
- **Preprocessing Utilities:** Includes scripts for data cleaning, transformation, and feature engineering.
- **Model Evaluation:** Tracks model performance with confusion matrices, feature importance, and evaluation metrics.
- **CI/CD Ready:** Compatible with CI/CD pipelines for automated testing and deployment.

---

## ğŸ–¼ï¸ System Architecture

### Network Intrusion Detection & Monitoring System Architecture
![System Architecture](results/images/System-Design.png)

This architecture illustrates the Vigilix pipeline:
1. **Data Ingestion:** Raw network data is ingested into Kafka topics.
2. **Data Processing:** Data is preprocessed and streamed to machine learning models for inference.
3. **Anomaly Detection:** Models classify data as normal or anomalous.
4. **Monitoring:** Prometheus collects metrics, and Grafana visualizes them in real-time dashboards.

---

## ğŸ–¥ï¸ Dashboard Example

### Vigilix Kafka Real-Time Monitoring Dashboard
![Dashboard Screenshot](results/images/Sample-Dashboard-Screenshot.png)

The dashboard provides real-time insights, including:
- Total network requests processed.
- Number of anomalies detected.
- Anomaly detection ratio.
- Model performance metrics (e.g., accuracy, precision, recall).

---

## ğŸ§­ Project Structure

```bash
atharsayed-vigilix/
â”œâ”€â”€ README.md                      # Project overview, setup instructions, and usage guide
â”œâ”€â”€ requirements.txt              # Python dependencies for the project
â”œâ”€â”€ models/                        # Model training, tuning, and inference scripts
â”‚   â”œâ”€â”€ app.py                     # Main entry point to run and evaluate models
â”‚   â”œâ”€â”€ hyper-xgb.py               # Hyperparameter tuning for XGBoost
â”‚   â”œâ”€â”€ isolation-forest.py        # Isolation Forest anomaly detection implementation
â”‚   â”œâ”€â”€ random-forest.py           # Random Forest classification model
â”‚   â””â”€â”€ xgboost_model.py           # XGBoost classification model
â”œâ”€â”€ monitoring/                    # Monitoring stack configuration for model/data pipeline
â”‚   â”œâ”€â”€ grafana/                   # Grafana setup for data visualization
â”‚   â”‚   â”œâ”€â”€ datasource.yaml        # Grafana data source configuration (e.g., Prometheus)
â”‚   â”‚   â””â”€â”€ vigilix_dashboard.json # Predefined dashboard for model and system metrics
â”‚   â””â”€â”€ prometheus/                # Prometheus setup for metrics scraping
â”‚       â””â”€â”€ prometheus.yml         # Configuration file for Prometheus scrape jobs
â”œâ”€â”€ results/                       # Output directory for EDA summaries and model evaluations
â”‚   â”œâ”€â”€ eda/                       # EDA result storage
â”‚   â”‚   â””â”€â”€ eda_summary.txt        # Summary of statistical and visual data insights
â”‚   â””â”€â”€ images/                    # Directory for storing system architecture and dashboard images
â”‚   â”‚   â”œâ”€â”€ Sample-Dashboard-Screenshot.png  # Dashboard screenshot
â”‚   â”‚   â””â”€â”€ System-Design.png                # System architecture diagram
â”‚   â””â”€â”€ models/                    # Model evaluation metrics and performance logs
â”‚       â”œâ”€â”€ isolationforest_results.txt   # Evaluation results for Isolation Forest
â”‚       â”œâ”€â”€ RandomForest_results.txt      # Evaluation results for Random Forest
â”‚       â”œâ”€â”€ XGBoost_results.txt           # Evaluation results for XGBoost
â”‚       â””â”€â”€ XGBoost_Tuned_results.txt     # Evaluation results after XGBoost tuning
â”œâ”€â”€ scripts/                       # Scripts to automate environment or service startup
â”‚   â”œâ”€â”€ start-kafka.bat            # Script to launch Zookeeper, Kafka broker, and topics
â”‚   â””â”€â”€ start-prometheus.bat       # Script to start Prometheus monitoring service
â”œâ”€â”€ src/                           # Core data processing logic and helper utilities
â”‚   â”œâ”€â”€ main.py                    # MAIN ORCHESTRATOR 
â”‚   â”œâ”€â”€ eda.py                     # Script to perform Exploratory Data Analysis
â”‚   â”œâ”€â”€ preprocess.py              # Data cleaning and transformation logic
â”‚   â””â”€â”€ utils.py                   # Common helper functions used across modules
â”œâ”€â”€ streaming/                     # Kafka-based streaming components
â”‚   â”œâ”€â”€ kafka_consumer.py          # Kafka consumer to receive and process streaming data
â”‚   â”œâ”€â”€ kafka_producer.py          # Kafka producer to send data to topics
â”‚   â””â”€â”€ synthetic-producer.py      # Kafka producer for synthetic data generation
â””â”€â”€ testing/                       # Unit and integration tests
    â””â”€â”€ test_app.py                # Tests for model pipeline and app logic
```

---

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8 or higher
- Docker and Docker Compose
- Java Runtime Environment (for Kafka)

### Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/AtharSayed/vigilix.git
   cd vigilix
   ```

2. Install Python dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Start the services using Docker Compose:
   ```bash
   docker-compose up
   ```

4. Start the orchestrator:
   ```bash
   python src/main.py
   ```

---

## ğŸ› ï¸ Components

### 1. **Data Pipeline**
- **Kafka Producer:** Streams synthetic or real network data to Kafka topics.
- **Kafka Consumer:** Consumes data from Kafka topics and sends it to the ML models for inference.

### 2. **Machine Learning Models**
- **Isolation Forest:** Unsupervised anomaly detection model.
- **Random Forest:** Supervised classification model.
- **XGBoost:** Gradient boosting model for high-performance classification.

### 3. **Monitoring Stack**
- **Prometheus:** Collects metrics from the pipeline and models.
- **Grafana:** Visualizes metrics in real-time dashboards.

---

## ğŸ“Š Results

### Exploratory Data Analysis (EDA)
- **Correlation Heatmap:** Visualizes relationships between features.
- **Label Distribution:** Shows the distribution of normal vs. anomalous data.
- **Attack Category Distribution:** Highlights the types of attacks in the dataset.

### Model Evaluation
- **Confusion Matrices:** Evaluate model performance.
- **Feature Importance:** Identify the most important features for classification.
- **Metrics:** Accuracy, precision, recall, and F1-score.

---

## ğŸ§ª Testing

### Unit Tests
Run test_app.py to see the results of the prediction There are 2 attack and 2 non attack payloads hardcoded in the fiile that will be send to the model and in return the model will predict whether its an attack or not .
```bash
pytest testing/
```

## ğŸ“ˆ Future Enhancements

- **Model Registry:** Implement a model registry (e.g., MLflow) to track model versions and metrics.
- **Data Validation:** Add schema validation for incoming data.
- **Scalability:** Deploy the pipeline on Kubernetes for horizontal scaling.
- **Advanced Models:** Integrate deep learning models for anomaly detection.
- **Security:** Add authentication and authorization for APIs and dashboards.

---

## ğŸ›¡ï¸ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:
1. Fork the repository.
2. Create a new branch for your feature or bug fix.
3. Submit a pull request with a detailed description of your changes.

---


## ğŸŒŸ Acknowledgments

- **Kafka:** For real-time data streaming.
- **Prometheus & Grafana:** For monitoring and visualization.
- **Scikit-learn & XGBoost:** For machine learning models.
- **UNSW-NB15 Dataset:** For providing the dataset used in this project.
